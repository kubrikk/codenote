---
layout: post
title: "Асимптотическая сложность"
author: "kubrikk"
permalink: limiting-behavior
katex: True
---

Математика и информатика (компьютерные науки) имеют разные подходы к решению задач. Математика фокусируется на доказательстве существования объектов или решений, в то время как информатика уделяет внимание тому, как именно эти объекты или решения могут быть получены, и что не менее важно — получены быстро и точно. 

Когда речь идет о комбинаторных задачах, полный перебор всех возможных вариантов может работать для небольших наборов данных, но быстро становится неприемлемым по мере увеличения размера входных данных. Например, если задача сводится к проверке всех перестановок чисел от $$1$$ до $$n$$, то количество проверяемых перестановок равно $$n!$$ ($$n$$-факториал), что делает полный перебор практически нереализуемым на больших значения $$n$$. Для того, чтобы быть уверенным в том, что описанное вычисление может быть произведено в адекватное время, необходимо уметь *оценивать* время выполнения программы. Кроме того, необходимо уметь оценивать и занимаемую программой память, чтобы быть уверенным в том, что вычислительная машина способна "посчитать" с точки зрения своего аппаратного обеспечения.

**Вычислительная сложность** — это функция зависимости затрачиваемых ресурсов, используемых при работе некоторого алгоритма, от размера входных данных. Под затрачиваемыми ресурсами обычно понимаются *память* или *время*, требующиеся для реализации вычисления.
​
Таким образом, под **асимптотическим анализом вычислительной сложности** алгоритма мы будем понимать использование асимптотического анализа для оценки поведения алгоритма с точки зрения занимаемой им для обработки данных *памяти* $$M(n)$$ и затрачиваемого *времени* $$T(n)$$.

​В некоторых случаях временную сложность алгоритма можно определить точно, в большинстве ситуаций поиск её точного значения не имеет смысла. Это связано с тем, что, во-первых, точная сложность зависит от выбора элементарных операций (например, можно учитывать количество арифметических операций и битовых операций), а во-вторых, с ростом размера входных данных влияние постоянных множителей и слагаемых низшего порядка в выражении для точного времени работы становится крайне незначительным.

---

## Асимптотическая сложность

Анализ работы алгоритма на больших объёмах входных данных и оценка скорости роста времени его выполнения приводят к понятию асимптотической сложности. Алгоритм с меньшей асимптотической сложностью считается более эффективным для всех входных данных, за исключением, возможно, случаев с данными малого размера.

Для асимптотического анализа используется ряд обозначений, но мы остановимся на трёх:
0. $$f(n) \in \Omega(g(n))$$ — ограничение функции $$f(n)$$ снизу функцией $$g(n)$$;
1. $$f(n) \in O(g(n))$$ — оганичение функции $$f(n)$$ сверху функцией $$g(n)$$;
2. $$f(n) \in \Theta(g(n))$$ — огранчение функции $$f(n)$$ снизу и сверху функцией $$g(n)$$.

### Ограничение снизу

Говорят, что **функция $$\pmb{f(n)}$$ ограничена снизу функцией $$\pmb{g(n)}$$** тогда и только тогда, когда существует константа $$\exists (с \in \mathbb{R}^+)$$ и некоторая точка $$\exists (n_0 \in \mathbb{R}^+)$$, такие что $$\forall (n > n_0)$$ будет верно, что $$c \times g(n) \leq f(n)$$. Данное отношение принято обозначать $$f(n) \in \Omega(g(n))$$. Более формально:


$$f(n) \in \Omega(g(n)) \Longleftrightarrow \exists(c, n_0 \in \mathbb{R}^+), \forall (n > n_0) : c \times g(n) \leq f(n).$$


<img src="{{site.baseurl}}/assets/images/limiting-behavior/omega.png" class="img-responsive" width="500">
<em>Рис 1. $$f(n) \in \Omega(g(n))$$</em>

​***Дополнительное примечание*:** Когда мы говорим о том, что функция $$g(n)$$ ограничивает снизу функцию $$f(n)$$, это означает, что функция $$f(n)$$ растёт хотя бы так же быстро как $$g(n)$$, — то есть, начиная с определенного момента, её значение никогда не станет меньше функции $$g(n)$$ в той же точке (Рис 1). Иными словами, начиная с некоторого значения $$n_0$$, значение функции $$f(n)$$ всегда будет больше или равно значению функции $$c \times g(n)$$, где $$c$$ — некоторая произвольная константа.

### Ограничение сверху

Говорят, что **функция $$\pmb{f(n)}$$ ограничена сверху функцией $$\pmb{g(n)}$$** тогда и только тогда, когда существует константа $$\exists (с \in \mathbb{R}^+)$$ и некоторая точка $$\exists (n_0 \in \mathbb{R}^+)$$, такие что $$\forall (n > n_0)$$ будет верно, что $$c \times g(n) \geq f(n)$$. Данное отношение принято обозначать $$f(n) \in O(g(n))$$. Более формально:

$$f(n) \in O(g(n)) \Longleftrightarrow \exists(c, n_0 \in \mathbb{R}^+), \forall (n > n_0) : c \times g(n) \geq f(n).$$

<img src="{{site.baseurl}}/assets/images/limiting-behavior/big-o.png" class="img-responsive" width="500">
<em>Рис 2. $$f(n) \in O(g(n))$$</em>

​***Дополнительное примечание*:** Когда мы говорим о том, что функция $$g(n)$$ ограничивает сверху функцию $$f(n)$$, это означает, что функция $$f(n)$$ растёт хотя бы так же быстро как $$g(n)$$, — то есть, начиная с определенного момента, её значение никогда не станет меньше функции $$g(n)$$ в той же точке (Рис 2). Иными словами, начиная с некоторого значения $$n_0$$, значение функции $$f(n)$$ всегда будет больше или равно значению функции $$c \times g(n)$$, где $$c$$ — некоторая произвольная константа.

### Ограничение снизу и сверху

Говорят, что **функция $$\pmb{f(n)}$$ ограничена снизу и сверху функцией $$\pmb{g(n)}$$** тогда и только тогда, когда существуют константы $$\exists (с_1, c_2 \in \mathbb{R}^+)$$ и некоторая точка $$\exists (n_0 \in \mathbb{R}^+)$$, такие что $$\forall (n > n_0)$$ будет верно, что $$c_1 \times g(n) \leq f(n) \leq c_2 \times g(n)$$. Данное отношение принято обозначать $$f(n) \in \Theta(g(n))$$. Более формально:

$$f(n) \in \Theta(g(n)) \Longleftrightarrow \exists(c_1, c_2, n_0 \in \mathbb{R}^+), \forall (n > n_0) : c_1 \times g(n) \leq f(n) \leq c_2 \times g(n).$$

<img src="{{site.baseurl}}/assets/images/limiting-behavior/theta.png" class="img-responsive" width="500">
<em>Рис 3. $$f(n) \in \Theta(g(n))$$</em>

​***Дополнительное примечание*:** Когда мы говорим о том, что функция $$g(n)$$ ограничивает сверху функцию $$f(n)$$, это означает, что функция $$f(n)$$ растёт хотя бы так же быстро как $$g(n)$$, — то есть, начиная с определенного момента, её значение никогда не станет меньше функции $$g(n)$$ в той же точке (Рис 2). Иными словами, начиная с некоторого значения $$n_0$$, значение функции $$f(n)$$ всегда будет больше или равно значению функции $$c \times g(n)$$, где $$c$$ — некоторая произвольная константа.

---

## Пример

Предположим, что в качестве решения задачи сортировки массива целых чисел было предложено следюущее решение:

{% highlight cpp  linenos %}
/**
 * Алгоритм сортировки массива `arr` длины `size`.
 */
void some_sort(int* arr, int size) {
    for (int i = 0; i < size - 1; i++) {
        for (int j = 0; j < size - 1 - i; j++) {
            if (arr[j] > arr[j + 1]) { swap(arr[j], arr[j + 1]); }
        }
    }
}
{% endhighlight %}

При анализе данного алгоритма легко посчитать количество шагов цикла (внутреннего цикла). Мы $$n-1$$ раз уменьшаем количество итераций внутреннего цикла, при первом проходе по которому происходит $$n-1$$ итераций, каждая следующая итерация внешнего цикла требует $$n-2$$, $$n-3$$ и т.д. до $$1$$. То есть время выполнения алгоритма можно оценить следующим образом: 

$$T(n) = (n - 1) + (n - 2) + ... + 2 + 1 = \frac{n \times (n - 1)}{2} = \frac{1}{2}n^2 - \frac{1}{2}n.$$
